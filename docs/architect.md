# System Architecture: Voice-Driven AI Coding Assistant

This document outlines the architecture for the voice-driven AI coding assistant, designed to run primarily locally on macOS (M3+) with optional remote capabilities, integrating various AI and automation features for a "vibe coding" experience.

## Core Components

1.  **Voice Input (macOS Frontend)**
    *   Captures audio input from the user.
    *   Likely a native Swift application for tight OS integration.

2.  **Speech-to-Text (STT)**
    *   Converts raw audio into text transcripts.
    *   **Primary:** Local model (e.g., OpenAI Whisper via `whisper.cpp` or Apple's Speech Framework) for privacy and offline use.
    *   **Optional:** Cloud-based STT (e.g., Google Cloud Speech-to-Text) as a fallback or for higher accuracy if needed.

3.  **Natural Language Processing (NLP) / Intent Parsing**
    *   Analyzes the text transcript to understand the user's command or intent (e.g., "generate code," "modify file," "find bug," "run script," "deploy").
    *   Can use local LLMs (via Ollama/llama.cpp) or simpler rule-based/keyword matching initially.

4.  **AI Core (Local LLM)**
    *   The central intelligence for code generation, modification, analysis, and troubleshooting.
    *   Runs local code-focused LLMs (e.g., CodeLLaMA, StarCoder, Mistral, Codestral) via Ollama or llama.cpp for performance on M3.
    *   Handles tasks like:
        *   Generating code snippets or entire files.
        *   Modifying existing code based on instructions (potentially using AST for precision).
        *   Debugging code based on errors or descriptions.
        *   Suggesting fixes based on console logs or screenshots.
        *   Detecting project tech stack.

5.  **Model Context Protocol (MCP) Layer (Optional but Recommended)**
    *   Acts as a standardized interface between the AI Core and various tools/data sources.
    *   Consists of:
        *   **MCP Client:** Integrated into the main application logic.
        *   **MCP Server(s):** Wrappers around tools (Docker, Git, Playwright, File System). Can run locally on the M3 or remotely on a hosted server.
    *   Provides portability (local/remote transparency) and modularity.

6.  **Virtualized Project Environment**
    *   A secure, isolated directory where all project files and operations are contained.
    *   **Implementation:** Python-based virtual filesystem logic (checking paths against `VIRTUAL_ROOT`) or Docker volume mounting. Prevents the AI from accessing or modifying files outside this scope.

7.  **Automated Version Control (Git)**
    *   Manages the project's history within the virtualized environment.
    *   Uses `GitPython` or Git CLI commands triggered after each significant AI action (file creation/modification).
    *   Supports local commits and optional push/pull to remote repositories (e.g., GitHub).

8.  **Tech Stack Environment Manager (Docker)**
    *   Detects the project's tech stack (using AI Core or heuristics).
    *   Automatically builds and runs a Docker container tailored to the stack (e.g., Python, Node.js).
    *   Mounts the `VIRTUAL_ROOT` into the container (`/app`).
    *   Manages container lifecycle (start, stop, restart).

9.  **Execution Engine**
    *   Runs code or commands generated by the AI.
    *   Executes securely within the appropriate environment:
        *   Inside the Docker container via `docker exec` for project code.
        *   Directly via `subprocess` (within the virtual root's safety checks) for local helper scripts.

10. **Browser Interaction (Playwright)**
    *   Automates a web browser (Chrome, Firefox, WebKit) to interact with running web applications.
    *   Captures console logs (errors, messages).
    *   Takes screenshots for visual analysis.
    *   Controlled via MCP or directly from the application logic.

## Data Flow Example (Code Generation)

1.  User speaks: "Create a Python function to sort a list."
2.  **Voice Input** captures audio.
3.  **STT** transcribes to text: "Create a Python function to sort a list."
4.  **NLP** identifies intent: `generate_code`, language: `Python`, description: `sort a list`.
5.  Request sent to **AI Core** (potentially via **MCP**).
6.  **AI Core** (local LLM) generates Python code.
7.  Code is saved to a file (`sort_list.py`) within the **Virtualized Project Environment** (path checked).
8.  **Automated Version Control** commits the new file with a relevant message.
9.  (Optional) If a Python **Docker Environment** is running, the change is synced/redeployed.

## Key Principles

*   **Local First:** Prioritize running components locally on the M3 for speed, privacy, and offline access.
*   **Safety:** Strict containment within the virtualized environment; no unintended system access.
*   **Automation:** Minimize manual steps through automatic versioning, environment setup, and deployment.
*   **Modularity:** Components interact through defined interfaces (especially if using MCP).
*   **User Experience:** Aim for a seamless, "vibe-driven" interaction model. 